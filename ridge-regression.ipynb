{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression techniques, a linear function is sought that models a series of datapoints as *accurately as possible*. The question remains how to define *accurate*. In the conventional linear regression technique, the sum of the squared errors forms a *loss function* that provides a metric as to what is an accurate. This loss function is conventional, but is not the only loss function that could be used.\n",
    "\n",
    "In this notebook another type of linear regression, called *ridge regression*, is used. This form of regression uses a similar loss function, but with the addition that small weights are prioritised over larger weights.\n",
    "\n",
    "In what follows, a dataset will be synthesised and then the technique of ridge regression will be applied to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model and the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model of the form:\n",
    "    \n",
    "$y = w \\cdot x$\n",
    "\n",
    "will be considered, where $x$ is a 100 dimensional vector and $w$ is a vector whose first 10 entries are 1 and its last 90 entries are zero:\n",
    "\n",
    "$w = [1,1,1,1,1,1,1,1,1,1,0,0,0,....0,0,0]$\n",
    "\n",
    "In order to generate data that approximately fits this model, points $(x^{(i)}, y^{(i)})$ can be chosen according to:\n",
    "\n",
    "$y^{(i)} = w \\cdot x^{(i)} + e^{(i)}$\n",
    "\n",
    "with $e^{(i)} \\sim N(\\mu=0, \\sigma=1)$. The additional $e^{(i)}$ represents Gaussian noise for the points that would otherwise be perfectly governed by the relationship $y = w \\cdot x$. \n",
    "\n",
    "Ideally in our linear regression, the vector $w$ that models these points will have the first 10 entries close to one and the last 90 entries close to zero. In order to implement this, a technique called ridge reression needs to be implemented. In ridge regression, the loss function can be written as:\n",
    "\n",
    "$L(w) = \\Sigma_{i} (y^{(i)} - (w_1 x_1^{(i)} + ... + w_d x_d^{(i)}))^2 \\ + \\alpha(w_1^2 + ... + w_d^2 )$\n",
    "\n",
    "where $\\alpha$ is a positive coefficient. The idea behind ridge regression is that weights that are smaller are prioritised because they reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the needed libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate the technique of ridge regression, a dataset needs to be created. The function `generate_data` does this. For example, the call `generate_data(number_of_points = 200, number_of_dimensions = 100)` returns 200 points of dimension 100 that are generated by $y^{(i)} = w \\cdot x^{(i)} + e^{(i)}$ with $e^{(i)} \\sim N(\\mu=0, \\sigma=1)$ and $w$ a vector whose first 10 entires are 1 and its remaining entries are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(number_of_points, number_of_dimensions):\n",
    "    \n",
    "    w = np.zeros(number_of_dimensions)\n",
    "\n",
    "    # Make the first 10 components of w equal to one. The rest will stay zero.\n",
    "    for i in range(10):\n",
    "        w[i]=1\n",
    "\n",
    "    X = np.random.normal(size=(number_of_points, number_of_dimensions)) \n",
    "    \n",
    "    y = np.dot(X, w) + np.random.normal(size = number_of_points)\n",
    "    \n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset can be generated by a call to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(number_of_points=200, number_of_dimensions=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test dataset comprising of 10% of the original data can be witheld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Ridge Regression to the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first effort, the alpha value in the loss function will be kept at a 1.0. The ridge regression model can be constructed by calling `Ridge` from `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "reg = Ridge(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients, $w$, of the model can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93175402,  1.03852502,  1.14688153,  1.14671353,  1.01591362,\n",
       "        0.96286916,  0.9908866 ,  0.74204582,  0.71007161,  1.00097678,\n",
       "        0.0086987 ,  0.07092882,  0.03005427,  0.05483077, -0.04690283,\n",
       "        0.05102834, -0.03554497, -0.14976768,  0.20439864, -0.018589  ,\n",
       "       -0.17657687,  0.06349957, -0.05238249, -0.05075177,  0.00454718,\n",
       "        0.19794064, -0.0259413 ,  0.01216122, -0.12173588, -0.01895408,\n",
       "        0.00495658,  0.02855822, -0.13432454, -0.02171785,  0.10204936,\n",
       "       -0.03197143, -0.20861917, -0.13102983, -0.02473363, -0.06612684,\n",
       "       -0.153332  , -0.0660188 , -0.00428363, -0.20590617,  0.10473827,\n",
       "       -0.26963921,  0.15522116,  0.11579502, -0.18243643,  0.13525745,\n",
       "        0.11902044, -0.11070771,  0.05947429, -0.01591181,  0.01338824,\n",
       "        0.17499702,  0.15132118, -0.08811159,  0.03037395, -0.01897913,\n",
       "       -0.00944754, -0.08404712,  0.02905173, -0.03501473,  0.04129698,\n",
       "       -0.05568717,  0.18383745, -0.02954668, -0.1118225 , -0.04998273,\n",
       "       -0.1152229 ,  0.00508036,  0.05528425,  0.09806355,  0.11463108,\n",
       "       -0.08332385,  0.02945876,  0.16034908,  0.11362113, -0.03551393,\n",
       "        0.0122884 ,  0.11490591, -0.08111778,  0.09568302, -0.05263268,\n",
       "        0.18572935,  0.09932973,  0.19013386, -0.09146975, -0.07670998,\n",
       "        0.172906  , -0.07980159, -0.04758168, -0.10932419, -0.00747901,\n",
       "        0.18070503,  0.01541324,  0.02239539,  0.2432497 , -0.15737123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the weights above, the first ten are close to one and the others are much smaller in magnitude. This is exactly what would be expected by the ridge regression technique that favours having smaller coefficients. \n",
    "Numerically, the mean and standard deviation of the first ten components is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9686637700858316, 0.1384687403099908)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_[0:10].mean(), reg.coef_[0:10].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the mean and standard deviation of the other components is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003472884305438479, 0.1081023903467146)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_[10:100].mean(), reg.coef_[10:100].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model can be used to predict the values y_train values. The *root mean square error* of these predictions can be determined using the `mean_squared_error` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the model on the training dataset is: 0.5880707073806555\n"
     ]
    }
   ],
   "source": [
    "y_predictions = reg.predict(X_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_predictions))\n",
    "\n",
    "print('The RMSE of the model on the training dataset is: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE on the train dataset is low which is to be expected. This is because the training data was used to build the model, hence the model should be able to predict the training data accurately. \n",
    "More importantly, the RMSE can be determined for the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the model on the test dataset is: 1.6166904106461708\n"
     ]
    }
   ],
   "source": [
    "y_predictions = reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predictions))\n",
    "\n",
    "print('The RMSE of the model on the test dataset is: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is larger on the test set, which is a sign that the underlying model is overfitted itself to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the alpha parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section it was assumed that the alpha constant was one. In this section, a grid search and the technique of cross validation is used to determine the ideal alpha constant.\n",
    "To begin, the ideal alpha constant's magnitude will be determined. In order to begin the search, a parameter grid is defined. This parameter grid space will use values of $10^{-4}$ to $10^4$ evenly distributed in logspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    " 'alpha': np.logspace(-4, 4, 9),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid can be created using `GridSearchCV` from `sklearn.model_selection`. This grid can be fit to the training data and then the best alpha parameter shown. Note: the loss function that is being used is the `neg_mean_squared_error` which is just the negative of the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 10.0}, score: -2.1698966232575434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1, return_train_score=False)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}, score: {}\".format(grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the best alpha value has an order of magnitude of around 10. As such,  a new parameter grid can be established that searches alpha values from 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters: {'alpha': 7}, score: -2.1593575141964356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    " 'alpha': np.arange(0, 100)    \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(Ridge(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1, return_train_score=False)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}, score: {}\".format(grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal alpha value for the training data is found to be 6. As such, a new ridge regression model can be created and trained on all the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=6, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 6\n",
    "reg = Ridge(alpha=alpha)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model can be used to predict the values y_train values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the model on the training dataset is: 0.606814219379828\n"
     ]
    }
   ],
   "source": [
    "y_predictions = reg.predict(X_train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_predictions))\n",
    "\n",
    "print('The RMSE of the model on the training dataset is: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the model on the test dataset is: 1.5365351160005354\n"
     ]
    }
   ],
   "source": [
    "y_predictions = reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predictions))\n",
    "\n",
    "print('The RMSE of the model on the test dataset is: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather annoyingly, both these errors are slightly larger than when the alpha value was guessed as 1.0. The effort to pick the alpha value using cross-validation did not make any improvement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a dataset was synthesised and a ridge regression model was applied to it. The model was able to produced weights that were expected of the underlying linear model that the data was sampled from. \n",
    "The root mean square errors of this model were determined, both for the training and test data sets. It was seen that the RMSE for the test data set was higher than for the training data set, as would be expected.\n",
    "Finally, the ideal alpha value of the ridge regression model was found using a gridsearch and cross validation. Unfortunately, this *ideal* alpha value did not improve either the RMSEs for the training nor test datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
